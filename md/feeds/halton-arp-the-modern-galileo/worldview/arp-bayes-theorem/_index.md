---
title: "On the Use of Bayes Theorem in Arguments Over Intrinsic Redshift"
controversy: "Halton Arp, the Modern Galileo"
discourse_level: "worldview"
authors: ["worldviewer"]
date: "2017-04-30"
lastmod: "2017-04-30"
project_url: ""
categories: ["astronomy", "astrophysics", "cosmology", "redshift", "quasars", "halton arp", "quasars, redshifts and controversies", "a posteriori", "bayes' theorem", "statistics"]
metrics: []
---

_Quasars, Redshifts and Controversies_
Halton Arp

_"In 1971, G. R. Burbidge, E. M. Burbidge, P. M. Solomon, and P. A. Strittmatter showed that among the quasars then known, those that fell very close on the sky to bright galaxies fell much closer than would be expected by chance. In a carefully worked-out statistical analysis, they showed that with even the few cases known from casual investigation, the chance that these closest coinicidences occurred accidentally was less than 5 x 10⁻³ or of the order of one in two hundred._

_The result was never criticized in print. As usual, however, it was excoriated in private. One of the major techniques for dismissing such results was introduced about this time. The catch-phrase is known as 'a posteriori statistics.' Normal people may not find that so catchy. But the idea is rather simple: After any event has happened, the probability of it happening in that precise way can always be computed to be vanishingly small. For example, if two people are photographed in the streets of a city of one million inhabitants, we would say that the chance of A being directly adjacent to B is one in a million. But in any random street scene there are perforce many A's next to unrelated B's. This is all quite evident from common sense. But what is also quite evident from common sense is, that if we continue to get photographs at different times and places of A next to B, we had better conclude some relationship exists between A and B. So far as the charge of 'a posteriori statistics' which has been levelled at each new piece of quasar evidence is concerned, the association of quasars with galaxies was demonstrated in 1966. Each succeeding example has therefore been an additional confirmation of an 'a priori' prediction. The dismissal of each of these on a case-by-case basis with the excuse of 'a posteriori statistics' has been, at best, poor science and, at worst, a tactic of evasion."_

http://electric-cosmos.org/Bayes.pdf

Statistics - Tool or Weapon?

_"The application and misapplication of Bayes’ theorem_

_There is an old saying: 'Statistics don't lie... but statisticians sometimes do.' This maxim is a corollary of the well known fact that 'Garbage In produces Garbage Out' (GIGO). It reaffirms the observation that, no matter how sophisticated any given computer program may be, if you feed it input data that is flawed, the output you will get will also be flawed. Despite this however, most people are usually impressed by complication: the more complicated the computerized algorithm, the more persuasive the results are for anyone who is gullible. The effectiveness of complicated statistics is a case in point._

_Most of us have heard the terms a priori and a posteriori and know they refer respectively to things that precede or follow an event. But most people do not know how to calculate nor to interpret an 'a posteriori probability' even though they may have occasionally heard that term. Such quantities are often used in science and we ought to know how to interpret them and how to judge the appropriateness of using them. For this we can turn to Bayes' theorem._

_Bayes' theorem (BT) is a statistical principle that is quite enlightening if properly applied. But as with many powerful tools, it can be dangerous if it is misused. BT was specifically developed to answer questions about the validity of tests. For example, consider the following test situations and then ask how much credence ought to be put in the results of these procedures:_

_1. Suppose you undergo a medical test for cancer. The test diagnoses you as having cancer. In this event, what is the probability that you indeed do have cancer?_

_2. A test is initiated to determine who is the best teacher in a school employing 30 teachers. This test has been previously shown to identify the best teacher from a similar sized group of teachers with 95% accuracy. After using this test to arrive at a winner, what is the probability we have correctly identified the best teacher on the faculty?_

_Bayes' Theorem provides a quantitative handle on the accuracy (believability) of answers to questions such as 1, and 2, (above). It lets us know how much faith we should put in the results of any given test when it is applied in some specific situation. But as with all mathematical procedures (computerized or not), we must be ever alert to the GIGO principle."_

[Please visit the link to Don Scott's site to view the mathematics here]

_"The moral of the story is this: If you are looking for the occurrence of something that is comparatively rare in the general population, your test has to be almost perfectly reliable or you will be fooling yourself if you believe the result you get from using it._

_So, if you are going to use a test (to try to calculate an a posteriori probability), always remember the following three rules:_

_1. The 'correct' conditional probabilities of the test, p(D|C) and p(not D|not C), must be extremely high._

_2. If the thing (event, quality) you are seeking to identify is relatively rare (generally occurs with low probability), or if you have only a very rough idea about the value of that a priori probability, you ought to re-think the question you are asking._

_3. Ask yourself whether the situation at hand actually calls for a probability computation at all or something different._

_This third rule (above) is a subtle but extremely important one. For example, consider the following story: One night two hikers camped out in a tent in Indonesia. They had a few drinks before retiring and so slept very soundly until one of them awoke abruptly and screamed. His companion jumped up and asked what was going on. The first camper yelled, 'A tiger just poked his head into our tent!' The second camper (being a good statistician) began to compute the a posteriori probability that a tiger had actually been there. The one who had seen the tiger yelled, 'I SAW THE DAMNED THING WITH MY OWN EYES! Don't start with the probability arguments, you dolt!'_

_This is, word-for-word, the answer that should be given in the following case as well:_

_An astronomer obtains an image of a highly redshifted object (QSO) that appears to be in front of a low redshifted galaxy. For example, see: http://www.thunderbolts.info/tpod/2004/arch/041001quasar-galaxy.htm. Other astronomers are unconvinced and demand that he should evaluate the a posteriori probability that the QSO is indeed closer to us than the galaxy._

_In this case, examining data is not a matter of 'probabilities' (neither a priori nor a posteriori). It is simply a question of do you believe the evidence or not. If not, then you must be prepared to say why not. Are you accusing the presenter of the evidence of counterfeiting it? Are you saying the QSO is an 'artifact' and not really there? To raise probabilistic arguments in cases where the evidence is 'in your face' is simply an evasion. It is dissembling. It is dishonest. When you have prima facie evidence of something, you do not need to initiate a 'test' to determine a posteriori probabilities. It is therefore incorrect to refer to 'a posteriori probabilities' when no test, as such, has been performed._

_For example, astronomer Halton Arp has presented a long series of images of unusual concentrations of BL Lac objects relatively near Seyfert galaxies. In order to quantify his observation, Arp calculated the average density of these objects (the number of them per square degree) over the entire sky. He then compared that BL Lac density measurement to their observed densities in small areas centered on Seyfert galaxies. He determined the ratio of those densities, to be > ~10,000. That is to say, the probability of finding a BL Lac near to a Seyfert galaxy is at least 10,000 times greater than the probability of finding one alone in an equal sized area chosen randomly on the open sky._

_Arp said¹ that one of his detractors snorted, 'Ridiculous!' He claimed no one would believe such a probability figure. And besides, 'it is a posteriori – computed after you found the effect.' This critic wanted to reject Arp's evidence -- he had no inclination to acknowledge its existence -- and he grabbed at the nearest straw he could find -- denouncing Arp's probability calculation as being an inaccurate a posteriori probability._

_First of all, it wasn't an a posteriori probability. As we have seen, an a posteriori probability quantifies how much you can trust the results presented by some kind of test. Arp was not performing any kind of test. He was simply comparing the observed density of a certain type object in one place on the sky contrasted with any other location. Arp 'saw the tiger in the tent with his own eyes' -- he did not need to perform any kind of test in order to verify those observations._

_Throwing around the descriptor 'a posteriori' in a pejorative attempt to belittle Arp's work, clearly demonstrates that the critic either does not understand probability theory – or hopes that we don't._

_But we do._

_Don Scott"_

--
¹ _Seeing Red_, pub Apeiron, Montreal, 1998, P.49.

http://www.holoscience.com/wp/whatever-happened-to-real-science/?article=a57ya4dj

_"Probabilities aren’t prices by which you can compare the apples and oranges of different initial beliefs. Probabilities incorporate the very initial beliefs that scientists should be discovering and questioning. The theory that is based on familiar assumptions will always calculate out as more probable than the ones with unfamiliar assumptions. Bayesian probabilities are little more than digitized familiarities. 'Secure knowledge' is the enemy of scientific discovery."_
